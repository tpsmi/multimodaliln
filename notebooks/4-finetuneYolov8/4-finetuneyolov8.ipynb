{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates how to train a YOLO (You Only Look Once) object detection model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we import the necessary library and set up our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pre-trained YOLO model\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we train the model on our custom dataset. The `train` method is used with specific parameters for our task. The .yaml file contains information about where the model should find the training images and labels. \n",
    "\n",
    "NB: training can take a long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the appropriate device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'  # For Mac with Apple Silicon\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'  # For Windows (or Linux) with NVIDIA GPU\n",
    "else:\n",
    "    device = 'cpu'  # Fallback to CPU for any system without GPU support\n",
    "\n",
    "# Use the determined device in the training function\n",
    "results = model.train(data='illustrationmodel.yaml', epochs=100, batch=16, imgsz=640, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming Training\n",
    "\n",
    "If training was interrupted, we can resume from the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from the last checkpoint\n",
    "model = YOLO(\"last.pt\")\n",
    "results = model.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export\n",
    "\n",
    "After training, we can export the model to ONNX format for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model to ONNX format\n",
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now let's evaluate the model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "metrics = model.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Results\n",
    "\n",
    "Let's look at some key metrics from our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overall metrics\n",
    "print(\"Precision:\", metrics.results_dict['metrics/precision(B)'])\n",
    "print(\"Recall:\", metrics.results_dict['metrics/recall(B)'])\n",
    "print(\"mAP50:\", metrics.results_dict['metrics/mAP50(B)'])\n",
    "print(\"mAP50-95:\", metrics.results_dict['metrics/mAP50-95(B)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics give us an overview of the model's performance:\n",
    "- Precision: The accuracy of positive predictions.\n",
    "- Recall: The fraction of actual positives that were identified.\n",
    "- mAP50: Mean Average Precision at 50% IoU.\n",
    "- mAP50-95: Mean Average Precision over different IoU thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "Let's create some visualizations to better understand our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract precision-recall curve data\n",
    "precision = metrics.curves[0][1]\n",
    "recall = metrics.curves[0][0]\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the trade-off between precision and recall. A curve that is closer to the top-right corner indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distribution\n",
    "confidence = metrics.curves[1][0]\n",
    "f1 = metrics.curves[1][1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence, f1)\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Confidence Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows how the F1 score (a balance of precision and recall) changes with different confidence thresholds. The peak of this curve can help in choosing an optimal confidence threshold for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've now trained a YOLO model, evaluated its performance, and visualized key metrics. These results can help in understanding the model's strengths and areas for improvement. Remember that model performance can often be enhanced by adjusting training parameters, augmenting the dataset, or using a different model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
