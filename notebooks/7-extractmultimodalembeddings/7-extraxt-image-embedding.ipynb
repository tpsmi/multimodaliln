{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Embedding Extraction with CLIP\n",
    "\n",
    "This notebook demonstrates how to extract image embeddings using the Open-CLIP (Contrastive Language-Image Pre-training) model. We'll go through the process step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, we need to import the necessary Python libraries. Each library has a specific purpose in our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # For neural network operations\n",
    "from transformers import CLIPProcessor, CLIPModel  # For using the CLIP model\n",
    "import glob  # For finding files matching a pattern\n",
    "from PIL import Image  # For opening and manipulating images\n",
    "from tqdm import tqdm  # For displaying progress bars\n",
    "import os  # For interacting with the operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Image Paths\n",
    "\n",
    "Next, we'll find all the JPEG images in a specified folder and its subfolders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source folder\n",
    "source_folder = \"/Volumes/Illustrated/data/extractedimages/illustratedweeklynews\"\n",
    "\n",
    "# Initialize an empty list to hold image paths\n",
    "image_paths = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for root, dirs, files in os.walk(source_folder):\n",
    "    # Collect all .jpg files in the current directory\n",
    "    image_paths.extend(glob.glob(os.path.join(root, \"*.jpg\")))\n",
    "\n",
    "# Sort the collected image paths\n",
    "image_paths = sorted(image_paths)\n",
    "\n",
    "# Print the number of collected image paths\n",
    "print(f\"Number of images found: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load CLIP Model and Processor\n",
    "\n",
    "Now we'll load the CLIP model and its associated processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model loaded and moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Helper Functions\n",
    "\n",
    "We'll define some helper functions to load and save embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return torch.load(file_path)\n",
    "    return None\n",
    "\n",
    "def save_embeddings(embeddings, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        existing_embeddings = torch.load(file_path)\n",
    "        embeddings = torch.cat([existing_embeddings, embeddings], dim=0)\n",
    "    torch.save(embeddings, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Main Embedding Extraction Function\n",
    "\n",
    "This function does the main work of extracting embeddings from our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_paths: list, processor, model, device, batch_size=32, checkpoint_path='embeddings.pt', save_every=100):\n",
    "    embeddings = load_embeddings(checkpoint_path)\n",
    "    start_index = len(embeddings) if embeddings is not None else 0\n",
    "    embeddings = [] if embeddings is None else [embeddings]\n",
    "\n",
    "    skipped_images = []\n",
    "\n",
    "    for i in tqdm(range(start_index, len(image_paths), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                image = Image.open(path)\n",
    "                image.load()\n",
    "                batch_images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping image '{path}' due to error: {str(e)}\")\n",
    "                skipped_images.append((path, str(e)))\n",
    "           \n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "            batch_embeddings = model.get_image_features(**inputs)\n",
    "            embeddings.append(batch_embeddings.cpu())\n",
    "\n",
    "        if (i // batch_size + 1) % save_every == 0:\n",
    "            save_embeddings(torch.cat(embeddings, dim=0), checkpoint_path)\n",
    "            embeddings = []\n",
    "\n",
    "    if embeddings:\n",
    "        save_embeddings(torch.cat(embeddings, dim=0), checkpoint_path)\n",
    "\n",
    "    # Save skipped images information\n",
    "    if skipped_images:\n",
    "        with open('skipped_images.txt', 'w') as f:\n",
    "            for path, error in skipped_images:\n",
    "                f.write(f\"{path}: {error}\\n\")\n",
    "\n",
    "    return load_embeddings(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the Embedding Extraction\n",
    "\n",
    "Now we'll run our function to extract embeddings from all our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "checkpoint_path = 'embeddings.pt'\n",
    "embeddings = get_image_embeddings(image_paths, processor, model, device, checkpoint_path=checkpoint_path)\n",
    "print(f\"Embeddings extracted and saved. Shape: {embeddings.shape}\")\n",
    "\n",
    "# Save final embeddings\n",
    "final_path = '/Volumes/Illustrated/code/multimodal/embeddings/OpenCLIPillustratedweeklynewsfull.pt'\n",
    "torch.save(embeddings, final_path)\n",
    "print(f\"Final embeddings saved to {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restarting Interrupted Embedding Extraction\\n\",\n",
    "\n",
    "If the embedding extraction process is interrupted (e.g., due to a power outage, system crash, or accidental notebook shutdown), you can easily restart it. The script is designed to resume from where it left off, thanks to the checkpoint system we've implemented. Here's how to restart the process:\n",
    "\n",
    "1. **Ensure all cells are executed**: Make sure all the previous cells in this notebook have been executed, including the import statements, function definitions, and model loading.\n",
    "2. **Check the checkpoint file**: Verify that the `embeddings.pt` file (or whatever name you've set for `checkpoint_path`) exists in your working directory. This file contains the embeddings that were successfully processed before the interruption.\n",
    "3. **Run the main execution cell again**: Simply re-run the cell above this explanation. The `get_image_embeddings` function will: Load the existing embeddings from the checkpoint file, determine how many images have already been processed, and start processing from the next unprocessed image.\n",
    "4. **Check for skipped images**: After the process completes, check if a `skipped_images.txt` file was created. This file lists any images that couldn't be processed, allowing you to investigate and potentially retry these specific images later.\n",
    " \n",
    "By following these steps, you can easily resume the embedding extraction process after an interruption without losing any progress. The script will continue from where it left off, ensuring all your images are processed efficiently.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to extract image embeddings using the CLIP model. The process involves loading images, processing them in batches, and saving the resulting embeddings. This can be useful for various downstream tasks such as image similarity search, clustering, or as input to other machine learning models.\n",
    "\n",
    "Remember to check the 'skipped_images.txt' file (if it was created) to see if any images were skipped during processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
