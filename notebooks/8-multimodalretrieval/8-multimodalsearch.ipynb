{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Search using CLIP\n",
    "\n",
    "This notebook demonstrates how to use the Open-CLIP (Contrastive Language-Image Pre-training) model to perform image similarity searches. We'll cover two types of searches:\n",
    "1. Text-to-Image: Find images that match a given text description\n",
    "2. Image-to-Image: Find images similar to a given image\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables (you can modify these as needed)\n",
    "os.environ['IMAGE_SOURCE_FOLDER'] = './data/images'\n",
    "os.environ['EMBEDDINGS_FILE'] = './embeddings/CLIPembeddings.pt'\n",
    "\n",
    "print(\"Libraries imported and environment variables set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the CLIP model\n",
    "\n",
    "Now, let's set up the CLIP model. This function will load the model and determine the best available device (GPU or CPU) for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_name):\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    return model, processor, device\n",
    "\n",
    "model_name = \"laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\n",
    "model, processor, device = setup_model(model_name)\n",
    "print(f\"Model loaded and set to use device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Image Dataset\n",
    "\n",
    "Next, we'll load the paths of all images in our dataset and their pre-computed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(source_folder):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return sorted(image_paths)\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "source_folder = os.environ.get('IMAGE_SOURCE_FOLDER', './data/images')\n",
    "embeddings_file = os.environ.get('EMBEDDINGS_FILE', './embeddings/CLIPembeddings.pt')\n",
    "\n",
    "image_paths = get_image_paths(source_folder)\n",
    "image_embeddings = load_embeddings(embeddings_file)\n",
    "\n",
    "print(f\"Loaded {len(image_paths)} image paths and their embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Now, let's define some utility functions that we'll use for our similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text, processor, model, device):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.get_text_features(**inputs)\n",
    "    return text_embedding.cpu()\n",
    "\n",
    "def get_image_embedding(image_path, processor, model, device):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.get_image_features(**inputs)\n",
    "        return image_embedding.cpu()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    embedding1 = embedding1 / embedding1.norm(dim=-1, keepdim=True)\n",
    "    embedding2 = embedding2 / embedding2.norm(dim=-1, keepdim=True)\n",
    "    return torch.matmul(embedding1, embedding2.T).squeeze()\n",
    "\n",
    "def retrieve_top_images(query_embedding, image_paths, image_embeddings, top_k=5):\n",
    "    similarities = compute_similarity(query_embedding, image_embeddings)\n",
    "    top_k_indices = torch.topk(similarities, min(top_k, len(similarities))).indices\n",
    "    top_k_image_paths = [image_paths[i] for i in top_k_indices]\n",
    "    top_k_similarities = [similarities[i].item() for i in top_k_indices]\n",
    "    return top_k_image_paths, top_k_similarities\n",
    "\n",
    "def display_images(image_paths, similarities, image_size=(5, 5), font_size=10, show_search_image=None):\n",
    "    def display_single_image(img_path, title):\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            plt.figure(figsize=image_size)\n",
    "            plt.imshow(np.array(image))\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"{title}\\nPath: {img_path}\", fontsize=font_size, wrap=True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image {img_path}: {e}\")\n",
    "    \n",
    "    if show_search_image:\n",
    "        display_single_image(show_search_image, \"Search Image\")\n",
    "    \n",
    "    for img_path, sim in zip(image_paths, similarities):\n",
    "        display_single_image(img_path, f\"Similarity: {sim:.4f}\")\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Image Search\n",
    "\n",
    "Now, let's perform a text-to-image search. This process involves:\n",
    "1. Converting the text query into an embedding using the CLIP model\n",
    "2. Comparing this text embedding with all the pre-computed image embeddings\n",
    "3. Retrieving the top matching images based on similarity scores\n",
    "\n",
    "You can modify the `text_query` to search for different concepts. You can change the number of top_k to retrieve more (or less) similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"a telegraph office\"\n",
    "top_k = 5\n",
    "text_embedding = get_text_embedding(text_query, processor, model, device)\n",
    "top_images, top_similarities = retrieve_top_images(text_embedding, image_paths, image_embeddings, top_k=top_k)\n",
    "\n",
    "print(f\"Top images matching the query '{text_query}':\")\n",
    "display_images(top_images, top_similarities, image_size=(8, 8), font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Image Search\n",
    "\n",
    "Now, let's perform an image-to-image search. This process involves:\n",
    "1. Loading a search image and converting it into an embedding using the CLIP model\n",
    "2. Comparing this image embedding with all the pre-computed image embeddings\n",
    "3. Retrieving the top matching images based on similarity scores\n",
    "\n",
    "You can modify the `search_image_path` to search for similar images to a different reference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_image_path = './testimages/parliament.jpg'\n",
    "top_k = 5\n",
    "search_image_embedding = get_image_embedding(search_image_path, processor, model, device)\n",
    "\n",
    "if search_image_embedding is not None:\n",
    "    similar_images, similarities = retrieve_top_images(search_image_embedding, image_paths, image_embeddings, top_k=top_k)\n",
    "    \n",
    "    print(f\"Top images similar to {search_image_path}:\")\n",
    "    display_images(similar_images, similarities, image_size=(8, 8), font_size=10, show_search_image=search_image_path)\n",
    "else:\n",
    "    print(\"Failed to process the search image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to use the CLIP model for both text-to-image and image-to-image similarity searches. You can modify the text queries or search images to explore different results within your image dataset.\n",
    "\n",
    "Remember that the quality of results depends on the diversity and relevance of your image dataset, as well as the pre-computed embeddings. If you want to use this notebook with your own image collection, you'll need to update the `IMAGE_SOURCE_FOLDER` and `EMBEDDINGS_FILE` environment variables, and ensure you have pre-computed CLIP embeddings for your images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
